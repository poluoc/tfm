{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Model libraries + others\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability libraries\n",
    "\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import shap\n",
    "import lime\n",
    "from ibreakdown import ClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_creditCardFraud.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data to Train Validation and test\n",
    "# We also drop feature time because it is different from every transacion and does not give us any insight\n",
    "X = data.drop(['Time'], axis=1)\n",
    "\n",
    "train, test = train_test_split(X, test_size=0.2, random_state=777)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np arrays for the features for the neural network\n",
    "train_labels = np.array(train.pop('Class'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val.pop('Class'))\n",
    "test_labels = np.array(test.pop('Class'))\n",
    "\n",
    "train_features = np.array(train)\n",
    "val_features = np.array(val)\n",
    "test_features = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to avoid peeking during tests\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(16, activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(data['Class'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(output_bias=initial_bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "und = pd.read_csv(\"cardFraud_X_train_undersampled.csv\")\n",
    "und_y = pd.read_csv(\"cardFraud_Y_train_undersampled.csv\")\n",
    "und['Class'] = und_y\n",
    "\n",
    "\n",
    "print(len(und))\n",
    "print(und['Class'].value_counts())\n",
    "und.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_und, val_und = train_test_split(und, test_size=0.10, random_state=777)\n",
    "\n",
    "# np arrays for the features for the neural network\n",
    "train_labels_und = np.array(train_und.pop('Class'))\n",
    "bool_train_labels_und = train_labels_und != 0\n",
    "val_labels_und = np.array(val_und.pop('Class'))\n",
    "\n",
    "train_features_und = np.array(train_und)\n",
    "val_features_und = np.array(val_und)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to avoid peeking during tests\n",
    "scaler = StandardScaler()\n",
    "train_features_und = scaler.fit_transform(train_features_und)\n",
    "val_features_und = scaler.transform(val_features_und)\n",
    "\n",
    "train_features_und = np.clip(train_features_und, -5, 5)\n",
    "val_features_und = np.clip(val_features_und, -5, 5)\n",
    "\n",
    "print('Training labels shape:', train_labels_und.shape)\n",
    "print('Validation labels shape:', val_labels_und.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features_und.shape)\n",
    "print('Validation features shape:', val_features_und.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 600 # we have far less data =(\n",
    "model_und = make_model()\n",
    "model_und.load_weights(initial_weights)\n",
    "baseline_history = model_und.fit(\n",
    "    train_features_und,\n",
    "    train_labels_und,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    #callbacks=[early_stopping],\n",
    "    validation_data=(val_features_und, val_labels_und))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_und = model_und.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results_und):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_und.save('creditCardFraud_tfnn_oversampledV2_b600.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = pd.read_csv(\"cardFraud_X_train_oversampled.csv\")\n",
    "over_y = pd.read_csv(\"cardFraud_Y_train_oversampled.csv\")\n",
    "over['Class'] = over_y\n",
    "\n",
    "\n",
    "print(len(over))\n",
    "print(over['Class'].value_counts())\n",
    "over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_over, val_over = train_test_split(over, test_size=0.10, random_state=777)\n",
    "\n",
    "# np arrays for the features for the neural network\n",
    "train_labels_over = np.array(train_over.pop('Class'))\n",
    "bool_train_labels_over = train_labels_over != 0\n",
    "val_labels_over = np.array(val_over.pop('Class'))\n",
    "\n",
    "train_features_over = np.array(train_over)\n",
    "val_features_over = np.array(val_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to avoid peeking during tests\n",
    "scaler = StandardScaler()\n",
    "train_features_over = scaler.fit_transform(train_features_over)\n",
    "val_features_over = scaler.transform(val_features_over)\n",
    "\n",
    "train_features_over = np.clip(train_features_over, -5, 5)\n",
    "val_features_over = np.clip(val_features_over, -5, 5)\n",
    "\n",
    "print('Training labels shape:', train_labels_over.shape)\n",
    "print('Validation labels shape:', val_labels_over.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features_over.shape)\n",
    "print('Validation features shape:', val_features_over.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250000\n",
    "\n",
    "model_over = make_model()\n",
    "model_over.load_weights(initial_weights)\n",
    "baseline_history_over = model_over.fit(\n",
    "    train_features_over,\n",
    "    train_labels_over,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(val_features_over, val_labels_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_results_over = model_over.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results_over):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_over.save('creditCardFraud_tfnn_oversampled_b250000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over2 = pd.read_csv(\"cardFraud_X_train_oversampled_v2.csv\")\n",
    "over2_y = pd.read_csv(\"cardFraud_Y_train_oversampled_v2.csv\")\n",
    "over2['Class'] = over2_y\n",
    "\n",
    "\n",
    "print(len(over2))\n",
    "print(over2['Class'].value_counts())\n",
    "over2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_over2, val_over2 = train_test_split(over2, test_size=0.10, random_state=777)\n",
    "\n",
    "# np arrays for the features for the neural network\n",
    "train_labels_over2 = np.array(train_over2.pop('Class'))\n",
    "bool_train_labels_over2 = train_labels_over2 != 0\n",
    "val_labels_over2 = np.array(val_over2.pop('Class'))\n",
    "\n",
    "train_features_over2 = np.array(train_over2)\n",
    "val_features_over2 = np.array(val_over2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to avoid peeking during tests\n",
    "scaler = StandardScaler()\n",
    "train_features_over2 = scaler.fit_transform(train_features_over2)\n",
    "val_features_over2 = scaler.transform(val_features_over2)\n",
    "\n",
    "train_features_over2 = np.clip(train_features_over2, -5, 5)\n",
    "val_features_over2 = np.clip(val_features_over2, -5, 5)\n",
    "\n",
    "print('Training labels shape:', train_labels_over2.shape)\n",
    "print('Validation labels shape:', val_labels_over2.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features_over2.shape)\n",
    "print('Validation features shape:', val_features_over2.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250000 # we have far less data =(\n",
    "\n",
    "model_over2 = make_model()\n",
    "model_over2.load_weights(initial_weights)\n",
    "baseline_history_over2 = model_over2.fit(\n",
    "    train_features_over2,\n",
    "    train_labels_over2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(val_features_over2, val_labels_over2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_over2 = model_over2.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results_over2):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_over2.save('creditCardFraud_tfnn_oversampledV2_b250000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = pd.read_csv(\"cardFraud_X_train_oversampled_syntethic.csv\")\n",
    "sin_y = pd.read_csv(\"cardFraud_Y_train_oversampled_syntethic.csv\")\n",
    "sin['Class'] = sin_y\n",
    "\n",
    "\n",
    "print(len(sin))\n",
    "print(sin['Class'].value_counts())\n",
    "sin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sin, val_sin = train_test_split(sin, test_size=0.10, random_state=777)\n",
    "\n",
    "# np arrays for the features for the neural network\n",
    "train_labels_sin = np.array(train_sin.pop('Class'))\n",
    "bool_train_labels_sin = train_labels_sin != 0\n",
    "val_labels_sin = np.array(val_sin.pop('Class'))\n",
    "\n",
    "train_features_sin = np.array(train_sin)\n",
    "val_features_sin = np.array(val_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to avoid peeking during tests\n",
    "scaler = StandardScaler()\n",
    "train_features_sin = scaler.fit_transform(train_features_sin)\n",
    "val_features_sin = scaler.transform(val_features_sin)\n",
    "\n",
    "train_features_sin = np.clip(train_features_sin, -5, 5)\n",
    "val_features_sin = np.clip(val_features_sin, -5, 5)\n",
    "\n",
    "print('Training labels shape:', train_labels_sin.shape)\n",
    "print('Validation labels shape:', val_labels_sin.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features_sin.shape)\n",
    "print('Validation features shape:', val_features_sin.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250000 \n",
    "\n",
    "model_sin = make_model()\n",
    "model_sin.load_weights(initial_weights)\n",
    "baseline_history_sin = model_sin.fit(\n",
    "    train_features_sin,\n",
    "    train_labels_sin,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(val_features_sin, val_labels_sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_sin = model_sin.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results_sin):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sin.save('creditCardFraud_tfnn_oversampledSin_b250000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainng NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_features[3398:3399]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the JS visualization code\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "explainer = shap.DeepExplainer(model_und,background)\n",
    "shap_values = explainer.shap_values(test_features[:100])\n",
    "print(\"Deep Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "explainer = shap.DeepExplainer(model_over,background)\n",
    "shap_values = explainer.shap_values(test_features[:100])\n",
    "print(\"Deep Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "explainer = shap.DeepExplainer(model_over2,background)\n",
    "shap_values = explainer.shap_values(test_features[:100])\n",
    "print(\"Deep Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "explainer = shap.DeepExplainer(model_sin,background)\n",
    "shap_values = explainer.shap_values(test_features[:100])\n",
    "print(\"Deep Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0].numpy().tolist(), shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "gradient_explainer = shap.GradientExplainer(model_und,background)\n",
    "shap_values, indexes = gradient_explainer.shap_values(test_features[:100], ranked_outputs=2)\n",
    "print(\"Gradient Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(0, shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(0, shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "gradient_explainer = shap.GradientExplainer(model_over,background)\n",
    "shap_values, indexes = gradient_explainer.shap_values(test_features[:100], ranked_outputs=2)\n",
    "print(\"Gradient Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(0, shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(0, shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "gradient_explainer = shap.GradientExplainer(model_over2,background)\n",
    "shap_values, indexes = gradient_explainer.shap_values(test_features[:100], ranked_outputs=2)\n",
    "print(\"Gradient Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(0, shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(0, shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:1000]\n",
    "gradient_explainer = shap.GradientExplainer(model_sin,background)\n",
    "shap_values, indexes = gradient_explainer.shap_values(test_features[:100], ranked_outputs=2)\n",
    "print(\"Gradient Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(0, shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(0, shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "kernel_explainer = shap.KernelExplainer(model_und.predict,background)\n",
    "shap_values = kernel_explainer.shap_values(test_features[:100])\n",
    "print(\"Kernel Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "kernel_explainer = shap.KernelExplainer(model_over.predict,background)\n",
    "shap_values = kernel_explainer.shap_values(test_features[:100])\n",
    "print(\"Kernel Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "kernel_explainer = shap.KernelExplainer(model_over2.predict,background)\n",
    "shap_values = kernel_explainer.shap_values(test_features[:100])\n",
    "print(\"Kernel Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "background = train_features[0:100]\n",
    "kernel_explainer = shap.KernelExplainer(model_sin.predict,background)\n",
    "shap_values = kernel_explainer.shap_values(test_features[:100])\n",
    "print(\"Kernel Explainer took: \" + str(time.time() - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explanations\n",
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(kernel_explainer.expected_value[0], shap_values[0][0,:], test_features[1016:1017][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_und(x):\n",
    "    predictions = model_und.predict(x)\n",
    "    y = 0\n",
    "    # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "    for i in predictions:\n",
    "        if y == 0: # priemra predicció, creem array de retu\n",
    "            res = np.array([[(1-i[0]),(i[0])]])\n",
    "            y = 1\n",
    "        else:\n",
    "            res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lime = lambda x: predict_und(x).astype(float)\n",
    "X = train_features[0:100]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(test_features,feature_names = train.columns,class_names=['Non-Fraud','Fraud'],kernel_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain_instance(sample[0], predictions_lime,num_features=len(test.columns))\n",
    "exp.show_in_notebook(show_all=False)\n",
    "tmp2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LIME took: \" + str(tmp2 - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_over(x):\n",
    "    predictions = model_over.predict(x)\n",
    "    y = 0\n",
    "    # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "    for i in predictions:\n",
    "        if y == 0: # priemra predicció, creem array de retu\n",
    "            res = np.array([[(1-i[0]),(i[0])]])\n",
    "            y = 1\n",
    "        else:\n",
    "            res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lime = lambda x: predict_over(x).astype(float)\n",
    "X = train_features[0:100]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(test_features,feature_names = data.columns,class_names=['Non-Fraud','Fraud'],kernel_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain_instance(sample[0], predictions_lime,num_features=len(test.columns))\n",
    "exp.show_in_notebook(show_all=False)\n",
    "tmp2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LIME took: \" + str(tmp2 - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_over2(x):\n",
    "    predictions = model_over2.predict(x)\n",
    "    y = 0\n",
    "    # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "    for i in predictions:\n",
    "        if y == 0: # priemra predicció, creem array de retu\n",
    "            res = np.array([[(1-i[0]),(i[0])]])\n",
    "            y = 1\n",
    "        else:\n",
    "            res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lime = lambda x: predict_over2(x).astype(float)\n",
    "X = train_features[0:100]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(test_features,feature_names = data.columns,class_names=['Non-Fraud','Fraud'],kernel_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain_instance(sample[0], predictions_lime,num_features=len(test.columns))\n",
    "exp.show_in_notebook(show_all=False)\n",
    "tmp2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LIME took: \" + str(tmp2 - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sin(x):\n",
    "    predictions = model_sin.predict(x)\n",
    "    y = 0\n",
    "    # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "    for i in predictions:\n",
    "        if y == 0: # priemra predicció, creem array de retu\n",
    "            res = np.array([[(1-i[0]),(i[0])]])\n",
    "            y = 1\n",
    "        else:\n",
    "            res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lime = lambda x: predict_sin(x).astype(float)\n",
    "X = train_features[0:100]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(test_features,feature_names = train.columns,class_names=['Non-Fraud','Fraud'],kernel_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain_instance(sample[0], predictions_lime,num_features=len(test.columns))\n",
    "exp.show_in_notebook(show_all=False)\n",
    "tmp2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LIME took: \" + str(tmp2 - tmp) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iBreakDown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini hackfix for ibreakdown to work for keras\n",
    "class predict_und_ibreakdown:\n",
    "    self = model_und\n",
    "    def predict_proba(x):\n",
    "        predictions = model_und.predict(x)\n",
    "        y = 0\n",
    "        # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "        for i in predictions:\n",
    "            if y == 0: # priemra predicció, creem array de retu\n",
    "                res = np.array([[(1-i[0]),(i[0])]])\n",
    "                y = 1\n",
    "            else:\n",
    "                res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ClassificationExplainer(predict_und_ibreakdown)\n",
    "classes = ['Non-Fraud', 'Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.fit(train_features[:100], test.columns, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_und.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain(sample)\n",
    "print(\"iBreakDown took: \" + str(time.time() - tmp) + \" seconds.\")\n",
    "print(model_und.predict(sample))\n",
    "exp.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini hackfix for ibreakdown to work for keras\n",
    "class predict_over_ibreakdown:\n",
    "    self = model_over\n",
    "    def predict_proba(x):\n",
    "        predictions = model_over.predict(x)\n",
    "        y = 0\n",
    "        # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "        for i in predictions:\n",
    "            if y == 0: # priemra predicció, creem array de retu\n",
    "                res = np.array([[(1-i[0]),(i[0])]])\n",
    "                y = 1\n",
    "            else:\n",
    "                res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ClassificationExplainer(predict_over_ibreakdown)\n",
    "classes = ['Non-Fraud', 'Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.fit(train_features[:100], test.columns, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_over.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain(sample)\n",
    "print(\"iBreakDown took: \" + str(time.time() - tmp) + \" seconds.\")\n",
    "print(model_over.predict(sample))\n",
    "exp.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini hackfix for ibreakdown to work for keras\n",
    "class predict_over2_ibreakdown:\n",
    "    self = model_over2\n",
    "    def predict_proba(x):\n",
    "        predictions = model_over2.predict(x)\n",
    "        y = 0\n",
    "        # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "        for i in predictions:\n",
    "            if y == 0: # priemra predicció, creem array de retu\n",
    "                res = np.array([[(1-i[0]),(i[0])]])\n",
    "                y = 1\n",
    "            else:\n",
    "                res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ClassificationExplainer(predict_over2_ibreakdown)\n",
    "classes = ['Non-Fraud', 'Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.fit(train_features[:100], test.columns, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_over2.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain(sample)\n",
    "print(\"iBreakDown took: \" + str(time.time() - tmp) + \" seconds.\")\n",
    "print(model_over2.predict(sample))\n",
    "exp.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini hackfix for ibreakdown to work for keras\n",
    "class predict_sin_ibreakdown:\n",
    "    self = model_sin\n",
    "    def predict_proba(x):\n",
    "        predictions = model_sin.predict(x)\n",
    "        y = 0\n",
    "        # Non-Fraud' = 0 ,'Fraud' = 1\n",
    "        for i in predictions:\n",
    "            if y == 0: # priemra predicció, creem array de retu\n",
    "                res = np.array([[(1-i[0]),(i[0])]])\n",
    "                y = 1\n",
    "            else:\n",
    "                res = np.concatenate((res, np.array([[(1-i[0]),(i[0])]])), axis=0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ClassificationExplainer(predict_sin_ibreakdown)\n",
    "classes = ['Non-Fraud', 'Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.fit(train_features[:100], test.columns, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sin.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time.time()\n",
    "exp = explainer.explain(sample)\n",
    "print(\"iBreakDown took: \" + str(time.time() - tmp) + \" seconds.\")\n",
    "print(model_sin.predict(sample))\n",
    "exp.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
